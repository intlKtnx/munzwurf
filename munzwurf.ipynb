{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from torch.utils import data\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        super().__init__()\n",
    "        self.train_data_cache = []\n",
    "        self.test_data_cache = []\n",
    "        self.transform = transform\n",
    "        self.label_count = [0,0,0,0,0,0,0]\n",
    "        # Search for all h5 files\n",
    "        p = Path(file_path)\n",
    "        files = p.glob('coinData_normalized')\n",
    "        for h5dataset_fp in files:\n",
    "            print(h5dataset_fp)\n",
    "            with h5py.File(h5dataset_fp.resolve()) as h5_file:\n",
    "                # Walk through all groups, extracting datasets\n",
    "                for gname, group in h5_file.items():\n",
    "                    k = 0\n",
    "                    j = 0\n",
    "                    if gname == 'oneCent':\n",
    "                        label = 0\n",
    "                    elif gname == 'twoCent':\n",
    "                        label = 1\n",
    "                    elif gname == 'fiveCent':\n",
    "                        label = 2\n",
    "                    elif gname == 'twentyCent':\n",
    "                        label = 3\n",
    "                    elif gname == 'fiftyCent':\n",
    "                        label = 4\n",
    "                    elif gname == 'oneEuro':\n",
    "                        label = 5\n",
    "                    elif gname == 'twoEuro':\n",
    "                        label = 6\n",
    "                    \n",
    "                    for dname, ds in tqdm(group.items()):\n",
    "                        \n",
    "                        arr = np.pad(np.array(ds, dtype=np.float32), (0, max(int(np.ceil(307200 - len(ds))), 0)))\n",
    "                        if k < 100:\n",
    "                            for i in np.array_split(arr[:307200], 300):\n",
    "                                if np.sum(i) != 0:\n",
    "                                    self.train_data_cache.append([label, torch.tensor(i).unsqueeze(0)])\n",
    "                            k += 1\n",
    "                        elif j < 25:\n",
    "                            for i in np.array_split(arr[:307200], 300):\n",
    "                                if np.sum(i) != 0:\n",
    "                                    self.test_data_cache.append([label, torch.tensor(i).unsqueeze(0)])\n",
    "                            j += 1\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_cache[index]\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.test_data_cache\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        return self.train_data_cache\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marcus/Dokumente/munzwurf/coinData_normalized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 130/130 [00:01<00:00, 108.45it/s]\n",
      "100%|█████████████████████████████████████████| 130/130 [00:01<00:00, 97.20it/s]\n",
      "100%|████████████████████████████████████████| 151/151 [00:01<00:00, 107.94it/s]\n",
      "100%|████████████████████████████████████████| 180/180 [00:01<00:00, 130.05it/s]\n",
      "100%|█████████████████████████████████████████| 137/137 [00:01<00:00, 97.86it/s]\n",
      "100%|████████████████████████████████████████| 170/170 [00:01<00:00, 131.57it/s]\n",
      "100%|████████████████████████████████████████| 196/196 [00:01<00:00, 126.41it/s]\n"
     ]
    }
   ],
   "source": [
    "customData = CustomDataset(\"/home/marcus/Dokumente/munzwurf/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44988\n",
      "195913\n"
     ]
    }
   ],
   "source": [
    "print(len(customData.get_test_data()))\n",
    "print(len(customData.get_train_data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_number = [0,0,0,0,0,0,0]\\ntest_number = [0,0,0,0,0,0,0]\\ntrain_data_idx = []\\ntest_data_idx = []\\nfor i in range(len(customData)):\\n    if train_number[customData[i][0]] < 30000:\\n        train_data_idx.append(i)\\n        train_number[customData[i][0]] += 1\\n    elif test_number[customData[i][0]] < 25*300:\\n        test_data_idx.append(i)\\n        test_number[customData[i][0]] += 1\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_number = [0,0,0,0,0,0,0]\n",
    "test_number = [0,0,0,0,0,0,0]\n",
    "train_data_idx = []\n",
    "test_data_idx = []\n",
    "for i in range(len(customData)):\n",
    "    if train_number[customData[i][0]] < 30000:\n",
    "        train_data_idx.append(i)\n",
    "        train_number[customData[i][0]] += 1\n",
    "    elif test_number[customData[i][0]] < 25*300:\n",
    "        test_data_idx.append(i)\n",
    "        test_number[customData[i][0]] += 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_labels = np.array([i[0] for i in customData], dtype = int)\\ntrain_labels = np.array([customData[i][0] for i in train_data_idx], dtype = int)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "test_labels = np.array([i[0] for i in customData], dtype = int)\n",
    "train_labels = np.array([customData[i][0] for i in train_data_idx], dtype = int)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = customData.get_train_data()\n",
    "test_data = customData.get_test_data()\n",
    "train_dataloader = DataLoader(train_data, batch_size=256, shuffle=True, pin_memory=False, num_workers=4)\n",
    "test_dataloader  = DataLoader(test_data, batch_size=32, shuffle=True, pin_memory=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlabel_count = []\\nfor i in range(7):\\n    label_count.append(np.count_nonzero(train_labels == i))\\n\\nplt.bar(range(7), label_count)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "label_count = []\n",
    "for i in range(7):\n",
    "    label_count.append(np.count_nonzero(train_labels == i))\n",
    "\n",
    "plt.bar(range(7), label_count)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1) \n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1) \n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, padding=1) \n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=3, padding=1) \n",
    "        self.conv5 = nn.Conv1d(128, 256, kernel_size=3, padding=1) \n",
    "        self.conv6 = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        \n",
    "        self.fc1   = nn.Linear(512, 7) \n",
    "     \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool1d(F.relu(self.conv1(x)),3)\n",
    "        #print(x.shape)\n",
    "        x = F.max_pool1d(F.relu(self.conv2(x)),3)\n",
    "        #print(x.shape)\n",
    "        x = F.max_pool1d(F.relu(self.conv3(x)),3)\n",
    "        #print(x.shape)\n",
    "        x = F.max_pool1d(F.relu(self.conv4(x)),3)\n",
    "        #print(x.shape)\n",
    "        x = F.max_pool1d(F.relu(self.conv5(x)),3)\n",
    "        x = F.max_pool1d(F.relu(self.conv6(x)),3)\n",
    "        #print(x.shape)\n",
    "        x = torch.flatten(x, 1) \n",
    "        #print(x.shape)\n",
    "        x = F.softmax(self.fc1(x), dim=1)\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv4): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv5): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv6): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (fc1): Linear(in_features=512, out_features=7, bias=True)\n",
      ")\n",
      "528423\n"
     ]
    }
   ],
   "source": [
    "model = Network().to(device)\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer, criterion, model):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    j = 0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [labels, inputs]\n",
    "        \n",
    "        inputs = data[1]\n",
    "        labels = data[0]        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 499:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 500), end=\" \")\n",
    "            running_loss = 0.0\n",
    "            if ((epoch+1) % 50) == 0:\n",
    "                print(outputs[1:10], labels[1:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(dataloader, optimizer, criterion, model):   \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for labels, inputs in dataloader:\n",
    "            labels, inputs = labels.to(device), inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   500] loss: 1.809 Test Error: Accuracy: 42.6%, Avg loss: 1.722968 \n",
      "[2,   500] loss: 1.546 Test Error: Accuracy: 51.6%, Avg loss: 1.630038 \n",
      "[3,   500] loss: 1.479 Test Error: Accuracy: 58.2%, Avg loss: 1.580998 \n",
      "[4,   500] loss: 1.439 Test Error: Accuracy: 59.6%, Avg loss: 1.564224 \n",
      "[5,   500] loss: 1.410 Test Error: Accuracy: 64.5%, Avg loss: 1.518919 \n",
      "[6,   500] loss: 1.387 Test Error: Accuracy: 67.3%, Avg loss: 1.488283 \n",
      "[7,   500] loss: 1.374 Test Error: Accuracy: 67.8%, Avg loss: 1.487830 \n",
      "[8,   500] loss: 1.365 Test Error: Accuracy: 64.8%, Avg loss: 1.510624 \n",
      "[9,   500] loss: 1.351 Test Error: Accuracy: 65.9%, Avg loss: 1.497693 \n",
      "[10,   500] loss: 1.344 Test Error: Accuracy: 68.7%, Avg loss: 1.474451 \n",
      "[11,   500] loss: 1.330 Test Error: Accuracy: 69.9%, Avg loss: 1.460921 \n",
      "[12,   500] loss: 1.324 Test Error: Accuracy: 71.2%, Avg loss: 1.449079 \n",
      "[13,   500] loss: 1.319 Test Error: Accuracy: 71.9%, Avg loss: 1.444130 \n",
      "[14,   500] loss: 1.314 Test Error: Accuracy: 69.1%, Avg loss: 1.472891 \n",
      "[15,   500] loss: 1.313 Test Error: Accuracy: 74.1%, Avg loss: 1.423202 \n",
      "[16,   500] loss: 1.311 Test Error: Accuracy: 70.9%, Avg loss: 1.453867 \n",
      "[17,   500] loss: 1.307 Test Error: Accuracy: 71.3%, Avg loss: 1.450512 \n",
      "[18,   500] loss: 1.305 Test Error: Accuracy: 71.0%, Avg loss: 1.453236 \n",
      "[19,   500] loss: 1.304 Test Error: Accuracy: 73.4%, Avg loss: 1.428415 \n",
      "[20,   500] loss: 1.298 Test Error: Accuracy: 69.6%, Avg loss: 1.466455 \n",
      "[21,   500] loss: 1.299 Test Error: Accuracy: 71.3%, Avg loss: 1.450707 \n",
      "[22,   500] loss: 1.300 "
     ]
    }
   ],
   "source": [
    "for epoch in range(500):  # loop over the dataset multiple times\n",
    "    train(train_dataloader, optimizer, criterion, model)\n",
    "    test(test_dataloader, optimizer, criterion, model)\n",
    "print('Finished Training')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
